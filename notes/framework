
===========

we need some framework we can train:
> ResNet
> Yolo
> VGG

===========

> So we cant use these quantization packages to train ? 
  > TensorFlow -> TFLITE Buffer
  > PyTorch dosnt do Integer only
  > capped out on post-train integer-only accuracy ? 

===========

> We can just be happy with 66% accuracy ? 
  > 69.4 -> 66 is HUGE fall off.
  > Acc w/ noise
  > Too sparse
  > VGG on CIFAR

===========

https://www.tensorflow.org/guide/distributed_training

> definitely think we should give this a go.
> means we likely have to create tfrecords tho ...

should test on cifar first tho to make sure we like it first.

> 150 img/sec
> this feels incredibly slow ...
> what is inference performance ? 
  > might be a better indication of how slow ...

8 V100s to train ResNet50 @ 120 minutes
> probably atleast 100x faster than ours

===========

> loading from HDD
> not using tensorflow input pipeline
> quantize 
> transforming weights each each batch ...

wonder if we setup this dataset if we can use the distributed training ...

===========

> write better stop conditions code.

> load.end()
  > give flag to each thread saying stop ? 
    > might not see shared memory ...
  
> while load.done() ...

===========

> running stuff now
> augment script to load in new weights and batch norm stuff.

> start the event based shit.

===========

> so why the fuck are we @ 68.5 right now ? 
  > had 69.6 at one point ...
  > YUP images on server must be corrupted lol.
  > running on laptop with higher accuracy.

> trained quantized network goes from 67.8->68.8 which is solid.

===========

On Local machine with good images:
> 69.3 -> 70.1 after training quantized network

===========

worried that that variance thing just worked ...
> we just averaged it and got it first time ...

> run collect script with quantization
> find largest scale value
> xcale is taken care of by batch norm

===========

we need to implement our idea for ResBlocks
> give more than 255b to layer with higher output.

===========

> yeah so once again res blocks are being the problem

initial idea was pass {qx, x} everywhere 
so we can just do 

x + y1
in resblock
rather than 
qx + qy1

but that dosnt work
because we scale the weights, but not the activations

===========

best new idea
is just finding the y_scale during training or something.

===========

hmm
what about passing y / scale through
and just doing it all during collection ? 

===========

so current setup is almost there
thinking the problem is collecting mean,var,scale at the same time.

because mean and var change as a function of scale.

===========

theory = cannot change scale and {mean, var} together.
need to do 2 separate collects

===========

> choose quantization values from collections
> collect mean and standard deviation

===========

so we figured out quantization I think.

> Why not do floating point for CC ?
> Why not just run the 8b matmul in PCM and then do the rest of stuff same as Pytorch ? 

if we want 8b everything, there are a few options

1)
scale value for {x, y2} {y2, y3} 
bc we scale all layers to (-128, 127) we lose information about how they can be combined.
so in the resblock u get fucked
cannot scale back by w, because what happens when you have {x, y2} ???

2)
> before we set a range for each layer based on largest output value in the network
> collected a ton of activations 1 layer by 1 layer
> ymax=14 -> instead of 40 so not everything got crushed by the max value

so if u want a good mehtod u need another format that all layers can be brought to and added together effectively

so fixed point or floating point @ 16-32 bits


===========


















