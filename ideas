
names:
smart skip: smart compute in-memory row skipping
speed read: dynamic compute in-memory 
Statistical dynamic variance
 
----

figure ideas:
> offset ?
> partitioning
> 4 pcm over 4 columns

ideas:
> download resnet/mobilenet/densenet weights
> LSTM
> we dont need to run actual sims, just log time it wud take
> only concerned with matrix mult.

----

> reference
> wl speed
> 8 fixed
> stat weight dist
> slow var

----

> profile on the fly.
> certain parts of x@w will be small ... spatial profiling
> for lstm, take advantage of time

LSTM = sparsity -> LSTM primary thing ? 
> argue PIM can inherently do sparsity better.

----

what can we do to make wstats work ? 
> increase wl
> decrease bl
> sparsity data
> params as f(x_bit, layer)

----

can we make a quantize_weight function
> that tries to quantize things to give us more zeros ? 

----

what about a different encoding format
where numbers will lots of 1s are 255 or something like this.
> this is the next step I believe.

----

> train network based on loss function where loss = count_ones()
> train network based on loss function that penalizes large positive values and small negative values

----

> might just want to work with lossy neural networks
> variance will kill us regardless, so might as well get some other values wrong as well.

> only redo it if its large bit/weight value ? 
> 7/7 or something ? 

----

we need to make batchnorm work I think
but its hard af in quantized networks.

----

> its better to have a low rpr/adc !!!
> basically increases length of wl 
  > nope then increases odds of getting that 8/8.

----

very interesting idea:
> quantization value = 100
> we know what our sums are at each wl.
> pick speed based on this information

> we go in the wrong order.
> start with b=7..0

> so u wud want to pass pim_kernel:
  > room for error
  > weight of errors for this bit

problem is you will never know room for errors until the very end.

----

the other thing = if you do rail out at 16/16 or w.e.
it eliminates variance at that high state which is nice.

----

> so we are told what is the number of errors we can tolerate ? 
> then we control adc read speed.

> specify normal distribution of variance you can tolerate at each neuron in the CNN.
> then we will hit that number, given everything else.
think this is a good idea.

----

> want to use tables for {RRAM/PCM}
  > where we specify all the parameters of the simulation
  > like those SNN papers kinda.
  
----










