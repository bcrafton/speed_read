
names:
smart skip: smart compute in-memory row skipping
speed read: dynamic compute in-memory 
Statistical dynamic variance
 
----

figure ideas:
> offset ?
> partitioning
> 4 pcm over 4 columns

ideas:
> download resnet/mobilenet/densenet weights
> LSTM
> we dont need to run actual sims, just log time it wud take
> only concerned with matrix mult.

----

> reference
> wl speed
> 8 fixed
> stat weight dist
> slow var

----

> profile on the fly.
> certain parts of x@w will be small ... spatial profiling
> for lstm, take advantage of time

LSTM = sparsity -> LSTM primary thing ? 
> argue PIM can inherently do sparsity better.

----

what can we do to make wstats work ? 
> increase wl
> decrease bl
> sparsity data
> params as f(x_bit, layer)

----

can we make a quantize_weight function
> that tries to quantize things to give us more zeros ? 

----

what about a different encoding format
where numbers will lots of 1s are 255 or something like this.
> this is the next step I believe.

----

> train network based on loss function where loss = count_ones()
> train network based on loss function that penalizes large positive values and small negative values

----

> might just want to work with lossy neural networks
> variance will kill us regardless, so might as well get some other values wrong as well.

> only redo it if its large bit/weight value ? 
> 7/7 or something ? 

----

we need to make batchnorm work I think
but its hard af in quantized networks.

----

> its better to have a low rpr/adc !!!
> basically increases length of wl 

----











