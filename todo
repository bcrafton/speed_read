
done
-------
> test bench
> slow conv reference (8 rows at a time)
> weight statistic dynamic speed
> variance
> layer-by-layer, bit-by-bit params
> how to hit target variance ? 
> how to make simulations go faster ?
> if pdot == adc:
  > pdot will always be larger
  > so we need to figure out what expected value is, if its larger than 16.
  > then add that much.
  > although expected value might be 16... so wudnt need to add in that case.
  
todo
-------
> dynamic profiling
> save {x,y1,y2,...} as tb from tensorflow.
> batchnorm
> is pooling necessary ?
> is there a relationship between the number of rows turned on and the 1s turned on in that row ? 
  store weight density of that row ... or some bit weighted density i.e. 7 more important than 1.

-------

> collection scripts
> bias centering
> cconv cleanup 

-------

> we dont do variance by cell.
  > we do read-to-read variance basically
  > wudnt really be useful for bias centering anyways.

> sweep no skip as well

> why this so slow ?  
  > https://stackoverflow.com/questions/10789042/python-multi-threading-slower-than-serial
  > apparently python has real issues with this ...

> better performance metric.

> move rpr out of conv.

-------

