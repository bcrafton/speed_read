
done
-------
> test bench
> slow conv reference (8 rows at a time)
> weight statistic dynamic speed
> variance
> layer-by-layer, bit-by-bit params
> how to hit target variance ? 
> how to make simulations go faster ?
> if pdot == adc:
  > pdot will always be larger
  > so we need to figure out what expected value is, if its larger than 16.
  > then add that much.
  > although expected value might be 16... so wudnt need to add in that case.

> why this so slow ?  
  > https://stackoverflow.com/questions/10789042/python-multi-threading-slower-than-serial
  > apparently python has real issues with this ...
  > multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.
  
> better performance metric.
  > nmac = (Hi * Wi) * (Fh * Fw * Ci * Co)
  > mwl = (Hi * Wi) * 8 * ((Fh * Fw * Ci) / WL) * (Co / BL)

> collection scripts

> move rpr out of conv

> is pooling necessary ?
  > yes.

future / not doing:
-------
> is there a relationship between the number of rows turned on and the 1s turned on in that row ? 
  store weight density of that row ... or some bit weighted density i.e. 7 more important than 1.

todo
-------
> dynamic profiling
> save {x,y1,y2,...} as tb from tensorflow.
> batchnorm

> bias centering
> cconv cleanup 
> we dont do variance by cell.
  > we do read-to-read variance basically
  > wudnt really be useful for bias centering anyways.
> no skip only go length of x ? 
  > currently it goes whole wl ...

> imagenet64

-------

priority:
> imagenet64

-------

model was wrong -> messed everything up.
> make sure Ho/Wo and yh/yw are correct.

as mean/std of activations/weights goes down, performance skyrockets.

-------

will we be able to support resnet/densenet/mobilenet ? 
> depthwise convolutions are bad for compute in memory.
> pointwise are just fine.

can we find online where state of the art is for 8 bit networks ? 
> what models are people using ? 
> what do accuracy numbers look like ? 

so it seems like everyone respects resnet.
which will be a giant pain to train.

> ResNet ? 
> BatchNorm
> Intermediate Accuracy #s would also be good.
> Parallelize over the 5 GPUs

-------

> run inference, make sure acc is good.
> parse thru code trying to find (nan, 1/0, numerical instability)

-------

















