
done
-------
> test bench
> slow conv reference (8 rows at a time)
> weight statistic dynamic speed
> variance
> layer-by-layer, bit-by-bit params
> how to hit target variance ? 
> how to make simulations go faster ?
> if pdot == adc:
  > pdot will always be larger
  > so we need to figure out what expected value is, if its larger than 16.
  > then add that much.
  > although expected value might be 16... so wudnt need to add in that case.

> why this so slow ?  
  > https://stackoverflow.com/questions/10789042/python-multi-threading-slower-than-serial
  > apparently python has real issues with this ...
  > multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.
> better performance metric.
  > nmac = (Hi * Wi) * (Fh * Fw * Ci * Co)
  > mwl = (Hi * Wi) * 8 * ((Fh * Fw * Ci) / WL) * (Co / BL)
> collection scripts
> move rpr out of conv
> is pooling necessary ?
  > yes.

future / not doing:
-------
> is there a relationship between the number of rows turned on and the 1s turned on in that row ? 
  store weight density of that row ... or some bit weighted density i.e. 7 more important than 1.

todo
-------
> make sure Ho/Wo and yh/yw are correct.
> ResNet ? 
> BatchNorm
> Intermediate Accuracy #s would also be good.
-------
> run inference, make sure acc is good.
> parse thru code trying to find (nan, 1/0, numerical instability)
-------
> better architecture sim
> device-to-device variance, not read-to-read
> there was 1 more thing ...
> 8 col/adc is good for x/w thing.
  > means we can pick our x/w pretty much however
  > also means we dont have to split at all.
-------

array allocation:
array: 222 cycle: 148 stall: 1548
array: 498 cycle: 381 stall: 52786
array: 240 cycle: 319 stall: 21096
array: 500 cycle: 251 stall: 28416
array: 240 cycle: 269 stall: 17496
array: 432 cycle: 173 stall: 18648

array: 222 cycle: 1280 stall: 22016
array: 498 cycle: 1664 stall: 42240
array: 240 cycle: 1664 stall: 6144
array: 500 cycle: 1408 stall: 48640
array: 240 cycle: 1408 stall: 10240
array: 432 cycle: 1408 stall: 18432

> going to be 2 parts:
1) (layer level) compile time based off of (X, W) statistics
2) (PE level) run time based on when arrays finish

found 2 techniques for doing this.
1) sparsity in x
2) mac/cycle/array
>> the winner will simply be decided by highest throughput of all 1024 arrays.

-------

Compile time array allocation based on layers.
> Seems like we solved this already.
> For report #1 we will just talk about this

Runtime allocation based on ?
Profile where stalls come from:
> When duplicate arrays have finished ? sync_dup
> Arrays in same PE have finished ? sync_array
Guessing it is probably the array, BUT can we duplicate things inside the PE to speed up array level stalls ?

> Need to make ndup configurable so we can make baseline work just as well.
> Lets see MAC/cycle.

-------

so i think we can formulate this better.
if u gather profiled data
and we assume that directly correlates to MACs, we shud be able to use some solver to compute the best configuration for throughput.

we shud be trying to use all the arrays tho.

-------

breaking barriers
> array level allocation
> this is going to be maximizing throughput around PD constraints

> identify the 3 bottlenecks
> talk about from a PD constraint we need to have 16 arrays in 1 block
> and the workarounds we have given these constraints.

-------

So the way I see it, there are really only two ways we can do this statically:

> increase number of duplications in PE to reduce idle time
> give more duplications to arrays that on average take longer

So we would have to profile input data to see if there is any trend, and we could look at covariance like to mentioned to see if there were any trend between arrays. 

-------

so we figured out how to solve these:
1. branch and bound = perfect
2. Binary ILP, where u unwind all the MAC/Array so u dont pick array(layer=0) = 5 ... u have that as a binay choice and can pick any of them. then u constrain ILP so that u can only pick 1 of those. Lastly, then u can maximize the minimum thing.
> https://math.stackexchange.com/questions/2446606/linear-programming-set-a-variable-the-max-between-two-another-variables/2447498

-------

thinking instead of these estimations, 
we just run the layers for different #narray in parallel
like 5-10 samples from each of reasonable window
then use that in the branch and bound.

-------

think we cannot separate it as layer/PE.
wud be better to view them together.
we can make use of additional arrays if we have them
dosnt need to be multiple of filter size.

may want to revert back a few commits ...

-------

> switch back 2 branches actually I think.

> yeah so we want to do block level allocation now
  > which means things are gonna change.
  > have to find best place to revert back to.
  
need to send Sam the .npy activation files as well.

-------

so how is block level allocation going to work ??
well every block is going to get some workload.
so we figure out how many MACs that is 
and then distribute it out basically.
same shit as layers except now we are doing blocks

yeah ... this is actually quite simple ...

> other things is ignore the idea of duplication, we just send whatever we can to whoever is next
this discussion really locked it down.

-------

1) pim.c API changes

2) branch and bound - tighter value function, needs to run faster ...
   > I think we can make our heuristic just taking the 100 best solutions in each step.

3) we are going to def have problems with partial blocks
   > 3x3x3x64
   > 3x3x64x64

-------

> find an upperbound on speed we can go.
  > (total 1s in all data) / (total arrays * 8)








