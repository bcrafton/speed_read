
done
-------
> test bench
> slow conv reference (8 rows at a time)
> weight statistic dynamic speed
> variance
> layer-by-layer, bit-by-bit params
> how to hit target variance ? 
> how to make simulations go faster ?
> if pdot == adc:
  > pdot will always be larger
  > so we need to figure out what expected value is, if its larger than 16.
  > then add that much.
  > although expected value might be 16... so wudnt need to add in that case.

> why this so slow ?  
  > https://stackoverflow.com/questions/10789042/python-multi-threading-slower-than-serial
  > apparently python has real issues with this ...
  > multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.
> better performance metric.
  > nmac = (Hi * Wi) * (Fh * Fw * Ci * Co)
  > mwl = (Hi * Wi) * 8 * ((Fh * Fw * Ci) / WL) * (Co / BL)
> collection scripts
> move rpr out of conv
> is pooling necessary ?
  > yes.

future / not doing:
-------
> is there a relationship between the number of rows turned on and the 1s turned on in that row ? 
  store weight density of that row ... or some bit weighted density i.e. 7 more important than 1.

todo
-------
> make sure Ho/Wo and yh/yw are correct.
> ResNet ? 
> BatchNorm
> Intermediate Accuracy #s would also be good.
-------
> run inference, make sure acc is good.
> parse thru code trying to find (nan, 1/0, numerical instability)
-------
> better architecture sim
> device-to-device variance, not read-to-read
> there was 1 more thing ...
> 8 col/adc is good for x/w thing.
  > means we can pick our x/w pretty much however
  > also means we dont have to split at all.
-------

> find an upperbound on speed we can go.
  > (total 1s in all data) / (total arrays * 8)
  > that is too low 
  > assume every array goes in 1 cycle.
  > yeah this really isnt too helpful ... its roughly 1300.

> fix the hacks = [block][bl]
> make sure this is correct

> be able to run {naive, layer-wise, block-wise}
  > plot them.

> need to run ResNet.

-------

priorities:
> how can we verify this shit ? 
  > clean up code
  > above all else, we need to make sure our shit is right.

> plots - with/without PE sync. allocation just on zero counts, not profiling.
> resnet

> compare with PE sync, account for PD constraints.
  > 1: 16 array per block ? 64 array per block ?
  > 2: the same blocks must be used together, i.e. you cannot just send any input data anywhere.

is there a way we can perform pim operations on the gpu ?
where compuation is same as our c kernel but we do some hack to run real fast on the gpu ? 
and still get metrics.

> building a robust simulator could be really useful.

> when we bring back pe_sync, verify it works same as 6115-7.

-------

ResNet18
> tb_resnet.py
> load imagenet data samples ~5 from validation set.
> need to support negative weights
> need to write code for residual blocks.

-------

> only explore branches that arnt +1 over upper bound. dosnt make sense otherwise.
> probably can do something similar with lower bound search.




















