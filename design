
-----

> how to structure stuff for comparing ? 
  > reference
  > wl speed
  > 8 fixed
  > stat weight dist
  > slow var

> ref -> separate function set.
> pim -> dictionaries for all params above.

-----

thinking much better design -> model based design where we call forward()
> but why didnt we do that before ? for emu ? 

-----

we messed up:
np.any(pdot == params['adc'])
> because this is supposed to be binary, but our weights are 4 bit.

we probably want 64 bits per array ? 128 is crazy ...
> we can have Sam make a figure about pitch matching and what not.
> 128 -> all 128 have to be less than 8.
  > 32, 64 much better i think.
  > give more than 8 states, then things really get crazy

-----

params = {
'bpa': 8,
'bpw': 8,
'rpr': 10,
'adc': 8,
'skip': 1
}

so we need to break this up.
bpw needed at both conv/pim level
is bpa ? 

dont really want to break up these annoying params
we cud just pass it to each layer.

lets do that for now.

-----

randomness in #pim comes from 2 things:
1) if using weight stats (of course)
2) in multi-layer networks, weights cause activation after layer 1 to be random

-----

> we are going to need to profile on the fly.
> because certain parts of the image and filter will not yield high results.
  > take advantage of space
> for lstm, take advantage of time

since LSTM sparsity maybe we can just use LSTM as primary thing
... wonder if we can argue PIM can inherently do sparsity better.

-----

real weights.
> need better repo for generating real weights.
> tf.floor/tf.clip kills gradient.

solutions
> tf.stop_gradient
> tf.quantization.quantize_and_dequantize
> https://github.com/tensorpack/tensorpack/issues/53
  '''
  E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))
  clip_x = tf.clip_by_value(x/E, -2.0, 2.0) / 2.0
  with G.gradient_override_map({"Floor": "Identity"}):
      return tf.round(clip_x) * E
  '''

these are really nice solutions for what we want to do

what about batchnorm ?
> do we need it or nah ?

okay we can make this happen.

-----

https://stackoverflow.com/questions/41391718/tensorflows-gradient-override-map-function
https://github.com/tensorpack/tensorpack/issues/53

lets start by replacing quantize_and_dequantize.

-----

[81008, 83808, 81440] = 8
wasnt 16 = 60000
what is up with that ...
[64160, 65864, 64936]

yeah wtf.
this is a totally different game than what i thought.

if 16 over 8 dosnt buy u a serious advantage ... then we have been doing something wrong. 

-----

setting wl = 1024
> 35520, 37240, 35656
what the fuck ??? 

definitely feels like we on to something here,
well a bug atleast
also seems like we need:
1) multi threading
2) writing this kernel in c or something.

something that is not so horribly slow.

-----

18536 @ 8/8
12296 @ 16/16

> we want to make 3x3x64x64 dominate the psum count
> so that means not using strides for this shit

-----

yeah so the first of our problems with weight stats
are this weird af not great benefit for 

756. 1152. 1166. @ 16
1208. 1416. 2010 @ 8

-----

[[ 6831.  6415. 13330.]] @ 12
[[4832. 5664. 8040.]] @ 8

[[4207. 4493. 9148.]] @ 10
[[3240. 4032. 6088.]] @ 8

-----

you might be able to go at 16->17 or 8->9 for free.
just because u can start at 17 if u failed.
nah because then the rest of ur numbers are fucked up.

-----

okay so now we can see the advantage to using 16 over 8
but is it though ? 
what are odds differences between 
20/16 vs 10/8 ? 

> take percentage of 1s columnwise
> odds of getting 16/20 in one of them
> how many times do u need to win for it to make up for 1 loss ?

-----

> time to add variance ???
> time to train networks to handle variance ?

[[2024. 3200. 3504.]] @ 16
[[1811. 3200. 3516.]] @ 20
> we only care about the large cases.
  > but why the fuck is col #1 so many pims ? 
  > (8h*8w*8b) * (27/16=2) * (32/4=8) 

okay but then why are there so few pims for the larger operations.

wow! there are so few 1s in these activations.
40/576 is probably the average.

when you make wl=1024
then it basically makes everything go as fast

64x32 takes same number of bitlines as 3x32.
just has slightly more ones.

-----

very interesting idea:
we are aware of quantization value = 100 probably or w.e.
and we know what our sums are.
so we can pick the speed to go at based on this information.

so to me, this means we go in the wrong order.

we wud want to start with b=7 and go down the line.

-----

okay so how do we make this work ? 
> we are told variance allowance. 
> we have variance of devices.
> we have quantization values

then how fast we can go
> (x_bit, w_bit, w_nonzero, w_var)
> dynamic stats
  > temporal, spatial
  > current y value ? knowing q value.

what can we compute in advance ? 
> expected error rate with static stats.

-----

we can use (1/4) C interfaces:
https://scipy-lectures.org/advanced/interfacing_with_c/interfacing_with_c.html

how can we make python code faster ? 
> parallelize it all.
> stick to 128x128 arrays.

-----

how do we do variance ? 
think it would be best.
to add variance after the fact.
yeah keep things to integers, then multiply by the number of states or w.e.
we know that any 1s are w.ON & x.ON

-----

something is not right here, we can read at crazy speeds.
solved it lol.

-----

okay we got a nice little setup here.
can see [min, max, mean, std] differences in feature maps.
> why is max never positive ? 
> y - y_ref
> well we only going faster ... so kinda makes sense.
  > so we should try using an offset here.

------

> how do we set rpr ? 
> pdot/adc offset
> fix quantize with that negative noise thing.

------

> to add variance, cant we just make it a function of the activation value ? 
> makes sense ... wud really just be function of on-states.

> y = y + np.random.normal(mean=0, std=params['sigma'] * np.sqrt(y), size=np.shape(y))

------

okay new questions now.
> how do we compute the correct number of rows to read ? 
> function(variance, adc, xbit, wbit(%))

------

> given all the col counts 
  > dont reduce max.
  > cant we come up with some average error everywhere ? 
    > means we dont do: samples=128 .. what is max std we shud expect.
  > yeah this sum thing i think is better ...

------

rpr error = binomial
for range(adc+1, rpr+1): 
> P(on state == 17 | rpr = 24) * (17 - 16)
instead of 
> for range(adc+1, rpr+1): 
maybe it should be for all states then.

------

https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6

> so the thing im worried about.
is we cud go even faster
and have valid c code.

https://cai.tools.sap/blog/how-to-speed-up-python-with-c/

ctypes is an option.
lets go with cython
seems like the clear path forward.
threading wud be my concern.

------

https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html

this is gonna suck to speedup because:
> y -= mu.reshape(oshape, bpw) @ shift

maybe we just write the whole thing in ctypes
really dont think it wud be that bad.

we wud need to just write the c code and a test bench
and make sure it works.

-----

and it really just needs to be the dot function.
still cant tell if it is worth the effort.
mkl gives nothing it seems.

we need to write c functions for:
> np.random.normal
> binom.pmf

might be too much effort.

-----

we dont really know how long these things will take anyways...
> havnt tried parllelizing top level workload.

-----

so we really just need to make this thing run faster.
so we are gonna re-code it
and probably use ctypes.

we can see what kind of speedup we can get
by tossing out variance and that stuff.

using it as an estimate before going further.

-----

> so basically we want speed vs variance plot.
> two different kinds of those plots but thats not important

we need to run 10 different variance for:
count = 1
count = 0

-----

how do we want this table structured ? 
> [examples] * layer * (var, cards)

can we plot all the layers on same graph ? 

-----

> threads still very slow. seems like python overhead (create_model, rpr, x/f transform) has huge overhead. 
> might want to copy x and send to each thread.

> why is std so high instantly ? 
  > def because we are using wrong var/rpr lut.

so its layer #3 we can see this issue occuring at var = 0.1
> then it starts to derail.
> hmmm ...
> well its actually the point after var=0.1 ... var=0.11.
> it dosnt look specific to layer 3 actuallly.
> what would this be then ???
  > so, the first wave = [5,6,7,8,9,10] * [0, 1]
  > 11 is just outside of that wave.
  > but we saw that before (1 at a time) anyways.

-----

author of that paper makes it pretty clear.
for training:
batch mean/std

for inference:
moving averages from training of mean/std.

so really it should not be that hard to come up with this ...

-----

we do this in quantize
> tf.clip_by_value(x, low, high)

feel like there shud not be a need for clip by value.

cifar10 accuracy.
fill in these damn sections with stuff.
pdlsyn figures, this we can do
batch norm was impossible.

-----

alright how do we use this quantize setup now ? 
> we want to put in a variance, and get the accuracy result right ? 
> so train a network, then see what accuracy it gets.
> the thing I would worry about is the floor function fucking things up.
  > how do we get true variance = 5 if it floors everything ? 
  > i suppose just put in 6? 

-----

> how can MAC/J be increasing ? 
> total MAC stays the same
> ADC, RON, ROFF change.

so we know we have less of ADC, RON, ROFF 
> RON / ROFF does not play a role at all.

changing variance inside of the c function "fixed" the problem.
which showed the thing is the activations going to layers deeper layers

BUT looking at this plot. i am seeing increasing energy efficieny for layer 1.
WTF !!!

> this narrows down our search. we can look at just the first layer!.

okay i think i might understand it
in the scenario where the WL is very sparse - 
we wud be turning on 8 VS turning on 1 or 2 or something
> min(rpr, WL - wl_ptr);
this would not save u.

so i think the way to show this is what is happening - atleast for layer 1, is min(pdot, rpr) 

-----

> duplicates need not be syncd.
  > until end of layer of course
> all arrays for a patch must be syncd
> layers must be syncd

> if one finishes before all the others, it can pick up on the remainder if its workload ...
  > no it cannot ...
  > each array has different weights. 

-----

> while(!done) instead of for r:R
  > cycle ++;
> need a different state machine for each one, 
> do we want to make a function for each array ? 

> we can ignore duplication to start ...
> for (int a=0; a<A; a++) { // do not actually need to create duplicate weights -> well for d2d variance we do.

-----

I think gameplan is to move cconv stuff into conv.
and do duplicates and stuff.
> this will essentially be what we did in 6115 already.
> so we will not have options to share weights between layers basically ? think im fine with that now.
  > dont think it wud be too hard to add in the future.

-----

> model - figures out how many duplicates each layer gets
> model.cut() -> call all layer.cut()
> layer.cut() -> create all arrays / duplicates. create rpr_lut, var_lut.
  > only think we dont do in cconv is the transformation of x stuff.

-----

> problem occurs between
  > seems mb correct 
  > starting dup 

-----

okay found the issue, its just that row=1023 finishes before something else and all else has to halt.

-----

alright what do we do now ? 
> cards
> baseline
> metrics
> need a better way of capturing dead cycles also.

-----












