
-----

> how to structure stuff for comparing ? 
  > reference
  > wl speed
  > 8 fixed
  > stat weight dist
  > slow var

> ref -> separate function set.
> pim -> dictionaries for all params above.

-----

thinking much better design -> model based design where we call forward()
> but why didnt we do that before ? for emu ? 

-----

we messed up:
np.any(pdot == params['adc'])
> because this is supposed to be binary, but our weights are 4 bit.

we probably want 64 bits per array ? 128 is crazy ...
> we can have Sam make a figure about pitch matching and what not.
> 128 -> all 128 have to be less than 8.
  > 32, 64 much better i think.
  > give more than 8 states, then things really get crazy

-----

params = {
'bpa': 8,
'bpw': 8,
'rpr': 10,
'adc': 8,
'skip': 1
}

so we need to break this up.
bpw needed at both conv/pim level
is bpa ? 

dont really want to break up these annoying params
we cud just pass it to each layer.

lets do that for now.

-----

randomness in #pim comes from 2 things:
1) if using weight stats (of course)
2) in multi-layer networks, weights cause activation after layer 1 to be random

-----

> we are going to need to profile on the fly.
> because certain parts of the image and filter will not yield high results.
  > take advantage of space
> for lstm, take advantage of time

since LSTM sparsity maybe we can just use LSTM as primary thing
... wonder if we can argue PIM can inherently do sparsity better.

-----

real weights.





























