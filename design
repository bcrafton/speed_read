
-----

> how to structure stuff for comparing ? 
  > reference
  > wl speed
  > 8 fixed
  > stat weight dist
  > slow var

> ref -> separate function set.
> pim -> dictionaries for all params above.

-----

thinking much better design -> model based design where we call forward()
> but why didnt we do that before ? for emu ? 

-----

we messed up:
np.any(pdot == params['adc'])
> because this is supposed to be binary, but our weights are 4 bit.

we probably want 64 bits per array ? 128 is crazy ...
> we can have Sam make a figure about pitch matching and what not.
> 128 -> all 128 have to be less than 8.
  > 32, 64 much better i think.
  > give more than 8 states, then things really get crazy

-----

params = {
'bpa': 8,
'bpw': 8,
'rpr': 10,
'adc': 8,
'skip': 1
}

so we need to break this up.
bpw needed at both conv/pim level
is bpa ? 

dont really want to break up these annoying params
we cud just pass it to each layer.

lets do that for now.

-----

randomness in #pim comes from 2 things:
1) if using weight stats (of course)
2) in multi-layer networks, weights cause activation after layer 1 to be random

-----

> we are going to need to profile on the fly.
> because certain parts of the image and filter will not yield high results.
  > take advantage of space
> for lstm, take advantage of time

since LSTM sparsity maybe we can just use LSTM as primary thing
... wonder if we can argue PIM can inherently do sparsity better.

-----

real weights.
> need better repo for generating real weights.
> tf.floor/tf.clip kills gradient.

solutions
> tf.stop_gradient
> tf.quantization.quantize_and_dequantize
> https://github.com/tensorpack/tensorpack/issues/53
  '''
  E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))
  clip_x = tf.clip_by_value(x/E, -2.0, 2.0) / 2.0
  with G.gradient_override_map({"Floor": "Identity"}):
      return tf.round(clip_x) * E
  '''

these are really nice solutions for what we want to do

what about batchnorm ?
> do we need it or nah ?

okay we can make this happen.

-----

https://stackoverflow.com/questions/41391718/tensorflows-gradient-override-map-function
https://github.com/tensorpack/tensorpack/issues/53

lets start by replacing quantize_and_dequantize.

-----

[81008, 83808, 81440] = 8
wasnt 16 = 60000
what is up with that ...
[64160, 65864, 64936]

yeah wtf.
this is a totally different game than what i thought.

if 16 over 8 dosnt buy u a serious advantage ... then we have been doing something wrong. 

-----

setting wl = 1024
> 35520, 37240, 35656
what the fuck ??? 

definitely feels like we on to something here,
well a bug atleast
also seems like we need:
1) multi threading
2) writing this kernel in c or something.

something that is not so horribly slow.

-----

18536 @ 8/8
12296 @ 16/16

> we want to make 3x3x64x64 dominate the psum count
> so that means not using strides for this shit

-----

yeah so the first of our problems with weight stats
are this weird af not great benefit for 

756. 1152. 1166. @ 16
1208. 1416. 2010 @ 8

-----

[[ 6831.  6415. 13330.]] @ 12
[[4832. 5664. 8040.]] @ 8

[[4207. 4493. 9148.]] @ 10
[[3240. 4032. 6088.]] @ 8

-----

you might be able to go at 16->17 or 8->9 for free.
just because u can start at 17 if u failed.
nah because then the rest of ur numbers are fucked up.

-----

okay so now we can see the advantage to using 16 over 8
but is it though ? 
what are odds differences between 
20/16 vs 10/8 ? 

> take percentage of 1s columnwise
> odds of getting 16/20 in one of them
> how many times do u need to win for it to make up for 1 loss ?

-----

> time to add variance ???
> time to train networks to handle variance ?

[[2024. 3200. 3504.]] @ 16
[[1811. 3200. 3516.]] @ 20
> we only care about the large cases.
  > but why the fuck is col #1 so many pims ? 
  > (8h*8w*8b) * (27/16=2) * (32/4=8) 

okay but then why are there so few pims for the larger operations.

wow! there are so few 1s in these activations.
40/576 is probably the average.

when you make wl=1024
then it basically makes everything go as fast

64x32 takes same number of bitlines as 3x32.
just has slightly more ones.

-----

very interesting idea:
we are aware of quantization value = 100 probably or w.e.
and we know what our sums are.
so we can pick the speed to go at based on this information.

so to me, this means we go in the wrong order.

we wud want to start with b=7 and go down the line.

-----

okay so how do we make this work ? 
> we are told variance allowance. 
> we have variance of devices.
> we have quantization values

then how fast we can go
> (x_bit, w_bit, w_nonzero, w_var)
> dynamic stats
  > temporal, spatial
  > current y value ? knowing q value.

what can we compute in advance ? 
> expected error rate with static stats.

-----

we can use (1/4) C interfaces:
https://scipy-lectures.org/advanced/interfacing_with_c/interfacing_with_c.html

how can we make python code faster ? 
> parallelize it all.
> stick to 128x128 arrays.

-----

how do we do variance ? 
think it would be best.
to add variance after the fact.
yeah keep things to integers, then multiply by the number of states or w.e.
we know that any 1s are w.ON & x.ON

-----

something is not right here, we can read at crazy speeds.
solved it lol.

-----

okay we got a nice little setup here.
can see [min, max, mean, std] differences in feature maps.
> why is max never positive ? 
> y - y_ref
> well we only going faster ... so kinda makes sense.
  > so we should try using an offset here.

------

> how do we set rpr ? 
> pdot/adc offset
> fix quantize with that negative noise thing.

------

> to add variance, cant we just make it a function of the activation value ? 
> makes sense ... wud really just be function of on-states.

> y = y + np.random.normal(mean=0, std=params['sigma'] * np.sqrt(y), size=np.shape(y))

------

okay new questions now.
> how do we compute the correct number of rows to read ? 
> function(variance, adc, xbit, wbit(%))

------

> given all the col counts 
  > dont reduce max.
  > cant we come up with some average error everywhere ? 
    > means we dont do: samples=128 .. what is max std we shud expect.
  > yeah this sum thing i think is better ...

------

rpr error = binomial
for range(adc+1, rpr+1): 
> P(on state == 17 | rpr = 24) * (17 - 16)
instead of 
> for range(adc+1, rpr+1): 
maybe it should be for all states then.

------

https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6

> so the thing im worried about.
is we cud go even faster
and have valid c code.

https://cai.tools.sap/blog/how-to-speed-up-python-with-c/

ctypes is an option.
lets go with cython
seems like the clear path forward.
threading wud be my concern.

------

https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html

this is gonna suck to speedup because:
> y -= mu.reshape(oshape, bpw) @ shift

maybe we just write the whole thing in ctypes
really dont think it wud be that bad.

we wud need to just write the c code and a test bench
and make sure it works.

-----

and it really just needs to be the dot function.
still cant tell if it is worth the effort.
mkl gives nothing it seems.

we need to write c functions for:
> np.random.normal
> binom.pmf

might be too much effort.

-----

we dont really know how long these things will take anyways...
> havnt tried parllelizing top level workload.

-----

so we really just need to make this thing run faster.
so we are gonna re-code it
and probably use ctypes.

we can see what kind of speedup we can get
by tossing out variance and that stuff.

using it as an estimate before going further.

-----

> so basically we want speed vs variance plot.
> two different kinds of those plots but thats not important

we need to run 10 different variance for:
count = 1
count = 0

-----

how do we want this table structured ? 
> [examples] * layer * (var, cards)

can we plot all the layers on same graph ? 

-----

> threads still very slow. seems like python overhead (create_model, rpr, x/f transform) has huge overhead. 
> might want to copy x and send to each thread.

> why is std so high instantly ? 
  > def because we are using wrong var/rpr lut.

so its layer #3 we can see this issue occuring at var = 0.1
> then it starts to derail.
> hmmm ...
> well its actually the point after var=0.1 ... var=0.11.
> it dosnt look specific to layer 3 actuallly.
> what would this be then ???
  > so, the first wave = [5,6,7,8,9,10] * [0, 1]
  > 11 is just outside of that wave.
  > but we saw that before (1 at a time) anyways.

-----

author of that paper makes it pretty clear.
for training:
batch mean/std

for inference:
moving averages from training of mean/std.

so really it should not be that hard to come up with this ...

-----

we do this in quantize
> tf.clip_by_value(x, low, high)

feel like there shud not be a need for clip by value.

cifar10 accuracy.
fill in these damn sections with stuff.
pdlsyn figures, this we can do
batch norm was impossible.

-----

alright how do we use this quantize setup now ? 
> we want to put in a variance, and get the accuracy result right ? 
> so train a network, then see what accuracy it gets.
> the thing I would worry about is the floor function fucking things up.
  > how do we get true variance = 5 if it floors everything ? 
  > i suppose just put in 6? 

-----

> how can MAC/J be increasing ? 
> total MAC stays the same
> ADC, RON, ROFF change.

so we know we have less of ADC, RON, ROFF 
> RON / ROFF does not play a role at all.

changing variance inside of the c function "fixed" the problem.
which showed the thing is the activations going to layers deeper layers

BUT looking at this plot. i am seeing increasing energy efficieny for layer 1.
WTF !!!

> this narrows down our search. we can look at just the first layer!.

okay i think i might understand it
in the scenario where the WL is very sparse - 
we wud be turning on 8 VS turning on 1 or 2 or something
> min(rpr, WL - wl_ptr);
this would not save u.

so i think the way to show this is what is happening - atleast for layer 1, is min(pdot, rpr) 

-----

> duplicates need not be syncd.
  > until end of layer of course
> all arrays for a patch must be syncd
> layers must be syncd

> if one finishes before all the others, it can pick up on the remainder if its workload ...
  > no it cannot ...
  > each array has different weights. 

-----

> while(!done) instead of for r:R
  > cycle ++;
> need a different state machine for each one, 
> do we want to make a function for each array ? 

> we can ignore duplication to start ...
> for (int a=0; a<A; a++) { // do not actually need to create duplicate weights -> well for d2d variance we do.

-----

I think gameplan is to move cconv stuff into conv.
and do duplicates and stuff.
> this will essentially be what we did in 6115 already.
> so we will not have options to share weights between layers basically ? think im fine with that now.
  > dont think it wud be too hard to add in the future.

-----

> model - figures out how many duplicates each layer gets
> model.cut() -> call all layer.cut()
> layer.cut() -> create all arrays / duplicates. create rpr_lut, var_lut.
  > only think we dont do in cconv is the transformation of x stuff.

-----

> problem occurs between
  > seems mb correct 
  > starting dup 

-----

okay found the issue, its just that row=1023 finishes before something else and all else has to halt.

-----

alright what do we do now ? 
> cards
> baseline
> metrics
> need a better way of capturing dead cycles also.

-----

okay, how do we compute this "performance loss" thing.

parameters:
> ncycle
> nstall 
> narray
> nmac

> mac_loss = (nstall / (narray * ncycle)) * mac_per_cycle
> utilization = (narray * ncycle) / (narray * ncycle + nstall)

-----

we still need to do layerwise synchonization problem.
> will make this much worse than currently is.

array: 222 cycle: 148 stall: 1548
array: 498 cycle: 381 stall: 52786
array: 240 cycle: 319 stall: 21096
array: 500 cycle: 251 stall: 28416
array: 240 cycle: 269 stall: 17496
array: 432 cycle: 173 stall: 18648

this is horrible.
381 cycles, why is it so bad ??

something tells me it has to do with zero skipping
> turn off skip and u will see exactly that.

array: 222 cycle: 1280 stall: 22016
array: 498 cycle: 1664 stall: 42240
array: 240 cycle: 1664 stall: 6144
array: 500 cycle: 1408 stall: 48640
array: 240 cycle: 1408 stall: 10240
array: 432 cycle: 1408 stall: 18432

-----

> has counting cards been broken ?
yeah it has to do with not reseting sat/pdot_sum

clear(sat);
clear(pdot_sum);
int wl_total = 0;
int wl_ptr = 0;
while (wl_ptr < WL) {

still didnt fix our problem...

>> can we expect that this has same results as paper ?
  > wud be nice debug.
  
oh wait, i think our weights are wrong ...
> nah then why wud it work with rpr=8.

are we doing rpr wrong ? 
like, we are not choosing correct rpr ? 

think i figured out the last piece
we wud do correction accross all wl
right now we are not doing that.
we are doing for each array.

... no actually i dont think that is right.

-----

well we have an assertion catching what I believe is the problem now.

nope thats not the problem either ...

-----

it has to be with rpr.
is xb weird at all ? 
> idts, it does not change accross the array. 

we cud go back to "6115-1" and try to make it work with the sat/pdot_sum fix
> wud tell us if it has anything to do with adc_ptr. 

'''
if (wl_total[array][d]) {
  float p = ((float) pdot_sum[d][array][bl_ptr]) / ((float) wl_total[d][array]);
'''
> holy shit lol

-----

alright so we can definitely work on array allocation
> i do wonder if Yu's paper discussed this in any more detail.

so allocation needs to be based on:
> MAC and MAC/cycle

previously we used MAC, but they operate at different speeds which is killing us.

so the question is how can we predict the number of MACs.
> def has to do with #zeros in activations and weights. 

-----

initial algorithm = 
mac/cycle/array = (% nonzero x) * (% nonzero w)
> well actually for zero-skipping w does not matter.

MAC = fixed

-----

> okay so hard coding ndup, we can definitely see that "good" ndup can be chosen.
  > can probably get 75% util this way accross the board

> going from 75% -> 90% is going to require the hard stuff.

-----

> share = f(network_mac, layer_mac, layer_mac_per_cycle)

so we did something that kinda works.

-----

> so our current solution works well using the sparsity in patches, dont even need to compute MAC/per cycle.

the winner will simply be decided by highest throughput of all 1024 arrays.

-----

> need to switch allocation based on algorithm:
  > baseline
  > zero skipping.

-----

> so we are only focusing on array level
ignore: array_sync, dup_done, dup_sync, r[d]
ignore: layer level

> if xb==7 and col==7
> array done = 1
> if all wl/bl are done, array sync
------
ignore these
------
> if all dups are done, dup sync
> if all layers are done, done
------

so when bl/wl finish, we need to repurpose them.
can we just do a neihbor thing ?

gameplan:
>>> idea for tonight, get the upper bound estimate
show how it correlates to MAC/cycle.

------

what are the loops at the array level:
> xb
> col
> wl 

yeah thats it
so either at the (xb, col) granularity we help out.

------

best if we do it in such a way
that they are getting summed together anyways.
but how ? 

if u wait till xb granularity, have a feeling there wont be enof to split.

------

array: 72 nmac 884736 cycle: 3528 stall: 3552
array: 354 nmac 9437184 cycle: 4261 stall: 412778
array: 132 nmac 4718592 cycle: 4604 stall: 164016
array: 240 nmac 9437184 cycle: 4107 stall: 209008
array: 120 nmac 4718592 cycle: 4178 stall: 124848
array: 144 nmac 9437184 cycle: 4071 stall: 137520
time taken: 8.473353147506714

array: 72 nmac 884736 cycle: 3528 stall: 3552
array: 354 nmac 9437184 cycle: 3187 stall: 32582
array: 132 nmac 4718592 cycle: 3414 stall: 6936
array: 240 nmac 9437184 cycle: 3295 stall: 14128
array: 120 nmac 4718592 cycle: 3196 stall: 7008
array: 144 nmac 9437184 cycle: 3127 stall: 1584
time taken: 8.821300745010376

------

so the upper bound is ridiculous
how can we get a realistic estimation ? 

like if we did only double sharing ? 
we tried this, results not so great. 

------

alright so how do we get that good utilization ? 
so I guess there are a couple options. 
> more duplicates
> smarter duplicates

------

so how many do we get ? 
that is one question.
but which ones get duplicates ? 
are there certain layers that just have less 1s ? 
> so thats actually not that much information to look at.

let me think.
so instead of the average patch
... each filter does a 128x16 chunk right ?  

so can we look at these patches
> and figure out a pattern of covariance
> or if any patches get on average less data. 

------

so we do this:
> x = nrow, nwl, wl, xb
> f = nwl, wl, nbl, bl
  
but i want to look at x as:
> (npatch, nwl, wl, nbl, bl, xb)

how do we break down each one ? 
or essentially each array inside each patch
and see if get any correlation at all.

we can create a standalone python script to do this.

one problem is we are gonna need to see this correlation over many images tho...
and part of me thinks that just isnt going to happen...

well it might happen in the channel dimension ?
> yeah thats the only thing I can think of here.

okay,
so how do we gather data ? 
well we get these weights from our quantize branch 
so we shud just pull all the activations then.

------

yeah so that worked out well
can clearly see on average some 128s have more data than others
so we need to allocate duplicates based on this.

------

alright so what is the plan ? 


------
































